# Layer-Wise Analysis Reveals Universal Discrete Emergence Across Large Language Model Architectures

Code for systematic investigation of capability emergence across large language model architectures.


## Overview

This repository maps where and how capabilities emerge within neural networks through layer-by-layer probing across 7 models and 4 cognitive tasks. Key findings include abrupt phase transitions, task-hierarchical formation, and architecture-invariant patterns.


---

## Setup Instructions

### 1. Clone the Repository

```bash
https://github.com/logicsame/Layer-Wise-Analysis-Reveals-Universal-Discrete-Emergence-Across-Large-Language-Model-Architectures.git
```

### 2. Navigate to the Project Directory

```bash
cd Layer-Wise-Analysis-Reveals-Universal-Discrete-Emergence-Across-Large-Language-Model-Architectures
```

### 3. Install The Framework

```bash
pip install -e .
```
### 4. Run The Experiment
```bash
python experiments/all_experiment.py # Note Required 48gb Gpu
```

### 5. Additional files informations
For ablation study provided the code in ablation_study folder as ablation.py and also provided the raw results in raw_results folder.
