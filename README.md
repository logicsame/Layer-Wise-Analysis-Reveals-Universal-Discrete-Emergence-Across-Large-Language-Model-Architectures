# Layer-Wise Analysis Reveals Universal Discrete Emergence Across Large Language Model Architectures

Code for systematic investigation of capability emergence across large language model architectures.



## Overview

This repository maps where and how capabilities emerge within neural networks through layer-by-layer probing across 7 models and 4 cognitive tasks. Key findings include abrupt phase transitions, task-hierarchical formation, and architecture-invariant patterns.

**Core contributions:**
- First systematic layer-wise emergence mapping framework
- Evidence for discrete vs. gradual capability formation  
- Universal task hierarchy across architectures
- 28 model-task combinations with statistical validation

See `core/` for mechanisms and `experiments/` for full benchmark code.